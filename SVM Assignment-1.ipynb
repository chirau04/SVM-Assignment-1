{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935342b4-3d08-4b2c-a7ce-ce6bb4ff86df",
   "metadata": {},
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) is typically represented as:\n",
    "\n",
    "f(x) = w^T * x + b\n",
    "\n",
    "Where:\n",
    "- f(x) is the decision function,\n",
    "- w is the weight vector perpendicular to the separating hyperplane,\n",
    "- x is the input vector,\n",
    "- b is the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3bd447-b902-43ac-bd76-0d64fe486095",
   "metadata": {},
   "source": [
    "The objective function of a linear SVM aims to maximize the margin between the two classes while minimizing the classification error. It's typically represented as:\n",
    "\n",
    "minimize: 1/2 * ⠟^2 + C * Σ(max(0, 1 - y_i * (w^T * x_i + b)))\n",
    "\n",
    "Where:\n",
    "- ⠵^2 represents the regularization term,\n",
    "- C is the regularization parameter,\n",
    "- (w^T * x_i + b) is the decision function,\n",
    "- y_i is the true class label of the i-th training sample,\n",
    "- x_i is the i-th training sample,\n",
    "- The summation is over all training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100091e8-9d11-47e3-935d-a5273aa396d6",
   "metadata": {},
   "source": [
    "The kernel trick in SVM is a technique used to handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional space. Instead of directly working in this higher-dimensional space, the kernel function computes the dot product between the mapped feature vectors, which is computationally more efficient. This allows SVMs to effectively find a hyperplane that separates the classes in the higher-dimensional space, even if the classes are not linearly separable in the original feature space. Popular kernel functions include the linear, polynomial, radial basis function (RBF), and sigmoid kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef759f7-a5a7-44e3-ad67-cf35d95d1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "*       *    *   *    *\n",
    "    *  *     *        *    \n",
    "   *       *   *   *  *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9b938-ad3e-4b2c-a020-510d62c2bbe1",
   "metadata": {},
   "source": [
    "Sure, let's illustrate these concepts with examples and graphs:\n",
    "\n",
    "1. Hyperplane: The hyperplane is the decision boundary that separates the classes in an SVM. In a binary classification problem, it's a line in 2D, a plane in 3D, and a hyperplane in higher dimensions. Here's an example of a hyperplane separating two classes in 2D:\n",
    "\n",
    "   ![Hyperplane](https://i.imgur.com/6sfD8rW.png)\n",
    "\n",
    "2. Margin: The margin is the distance between the hyperplane and the closest data points (support vectors). In a hard margin SVM, the margin is maximized. In a soft margin SVM, the margin is allowed to be violated by some data points to achieve a better overall classification. Here's an example of a margin in a hard margin SVM:\n",
    "\n",
    "   ![Margin](https://i.imgur.com/6z5iWM0.png)\n",
    "\n",
    "3. Soft Margin: In a soft margin SVM, the margin can be violated by some data points to allow for more flexibility in handling noisy data or overlapping classes. Here's an example of a soft margin SVM with some data points within the margin:\n",
    "\n",
    "   ![Soft Margin](https://i.imgur.com/9bDfZt4.png)\n",
    "\n",
    "4. Hard Margin: In a hard margin SVM, no data points are allowed to violate the margin. This makes the classifier more sensitive to outliers and noisy data. Here's an example of a hard margin SVM where all data points lie outside the margin:\n",
    "\n",
    "   ![Hard Margin](https://i.imgur.com/ueyARH3.png)\n",
    "\n",
    "These visualizations should help clarify the concepts of hyperplane, margin, soft margin, and hard margin in SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9cb2db-752f-480f-b218-7722e155e536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
